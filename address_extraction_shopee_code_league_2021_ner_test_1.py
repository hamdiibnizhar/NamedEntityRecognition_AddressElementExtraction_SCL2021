# -*- coding: utf-8 -*-
"""address-extraction-shopee-code-league-2021-NER-test-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y8_tdrWk-gH3ipFhue24xQSZ6CTqpZqs
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.chdir('/content/gdrive/My Drive/Projects')

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

!cp /content/gdrive/MyDrive/Datasets/scl-2021-ds.zip /content/
# !cp /content/gdrive/MyDrive/Datasets/scl-2021-ds.zip /content/

import zipfile
 
local_zip = '/content/scl-2021-ds.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

!pip install tensorflow-addons
!pip install transformers
!pip install seqeval

import os
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import sklearn

import tensorflow as tf
import tensorflow_addons as tfa
import keras_preprocessing
from keras_preprocessing import image
from keras_preprocessing.image import ImageDataGenerator
import tensorflow.keras.optimizers
import tensorflow_hub as hub

from sklearn.model_selection import train_test_split

from transformers import BertTokenizer, TFBertModel

model_name='cahya/bert-base-indonesian-522M'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = TFBertModel.from_pretrained(model_name)
text = "Silakan diganti dengan text apa saja."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)

dataset = pd.read_csv("/content/train.csv")
dataset

import re           
from bs4 import BeautifulSoup 
from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences
import nltk
from nltk.corpus import stopwords   
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
import warnings
pd.set_option("display.max_colwidth", 200)
warnings.filterwarnings("ignore")

nltk.download('stopwords')
stop_words = set(stopwords.words('indonesian')) 
nltk.download('punkt')

dataset['POI'] = dataset['POI/street'].str.extract(r'(.*)/', expand=True)
dataset['street'] = dataset['POI/street'].str.extract(r'/(.*)', expand=True)

dataset

def find_first_index(ra_split, st_split):
    
    if len(st_split) <= len(ra_split): # new
    
        num_iter = len(ra_split) - len(st_split) + 1
        overlap_list = []

        for i in range(num_iter):
            window = ra_split[i: i+len(st_split)]
            overlap = list(set(window) & set(st_split))
            overlap_list.append(len(overlap))

        max_overlap = [e for e in range(len(overlap_list)) if overlap_list[e] == max(overlap_list)]
        if len(max_overlap) == 1:
            return max_overlap[0]

        else:
            count_list = []
            for idx in max_overlap:
                subset_ra = ra_split[idx: idx+len(st_split)]
                count = 0
                for e in range(len(subset_ra)):
                    if subset_ra[e] not in st_split[e]:
                        count += 0
                    else:
                        count += 1
                count_list.append(count)
            index = count_list.index(max(count_list))
            return max_overlap[index]
    else:
        return 0


def fix_street_errors(row):
    
    raw_add = row['raw_address']

    # If a street name is extracted...
    if row['street'] != "":

        raw_add_split = nltk.tokenize.word_tokenize(raw_add)
        
        extr_street = row['street']
        extr_street_split = nltk.tokenize.word_tokenize(extr_street)
        
        # If the extracted street is in the raw address as an entire string, good!
        if extr_street in raw_add:
            return raw_add
        
        # This is where there are discrepancies!
        else:
            index_in_ra = find_first_index(raw_add_split, extr_street_split)
            raw_add_split[index_in_ra: index_in_ra+len(extr_street_split)] = extr_street_split
            updated_raw_add = ' '.join(raw_add_split).replace(' ,', ',').replace(' .', '.').replace(' )', ')').replace(' (', '(').replace(' ?', '?')          
            return updated_raw_add
      
    # If a street name is originally an empty string, we just assume there's no error. 
    else:
        return raw_add

dataset['cleaned_raw_address'] = dataset.apply(fix_street_errors, axis=1)

def fix_poi_errors(row):
    
    raw_add = row['cleaned_raw_address']

    # If a POI name is extracted...
    if row['POI'] != "":

        raw_add_split = nltk.tokenize.word_tokenize(raw_add)
        
        extr_poi = row['POI']
        extr_poi_split = nltk.tokenize.word_tokenize(extr_poi)
        
        # If the extracted POI is in the raw address as an entire string, good!
        if extr_poi in raw_add:
            return raw_add
        
        # This is where there are discrepancies!
        else:
            index_in_ra = find_first_index(raw_add_split, extr_poi_split)
            raw_add_split[index_in_ra: index_in_ra+len(extr_poi_split)] = extr_poi_split
            updated_raw_add = ' '.join(raw_add_split).replace(' ,', ',').replace(' .', '.').replace(' )', ')').replace(' (', '(').replace(' ?', '?')          
            return updated_raw_add
      
    # If a POI name is originally an empty string, we just assume there's no error. 
    else:
        return raw_add

dataset['cleaned_raw_address'] = dataset.apply(fix_poi_errors, axis=1)
dataset

"""#Train"""

import string

table = str.maketrans('', '', string.punctuation)

tag = []
for index, row in dataset.iterrows():
# for i in dataset['cleaned_raw_address']:
    temp = []
    # addr_elements = nltk.tokenize.word_tokenize(row['cleaned_raw_address']) #i.split()
    # street_element = nltk.tokenize.word_tokenize(row['street'])
    # POI_element = nltk.tokenize.word_tokenize(row['POI'])
    addr_elements = [w.translate(table) for w in row['cleaned_raw_address'].split()]
    # addr_elements = row['cleaned_raw_address'].split()
    street_element = [w.translate(table) for w in row['street'].split()]
    POI_element = [w.translate(table) for w in row['POI'].split()]
    for element in addr_elements:
        if element in street_element:
            temp.append('street')
        elif element in POI_element:
            temp.append('POI')
        else :
            temp.append('O')
    
    tag.append(temp)

dataset['tag'] = tag

import string

table = str.maketrans('', '', string.punctuation)
address_array = []
for index, row in dataset.iterrows():
    stripped = [w.translate(table) for w in row['cleaned_raw_address'].split()]
    address_array.append(stripped)
    
dataset['address_array'] = address_array

# address_array_all = address_array + test_address_array
# df = pd.DataFrame(list(zip(address_array_all)), columns =['address_array'])

from itertools import chain
def get_dict_map(data, token_or_tag):
    tok2idx = {}
    idx2tok = {}
    
    if token_or_tag == 'token':
        # vocab = list(set(dataset['cleaned_raw_address'].to_list()))
        temp = []
        for i in dataset['address_array'].to_list():
            temp.extend(list(set(i)))
        vocab = list(set(temp))
    else:
        # vocab = list(set(dataset['tag'].to_list()))
        temp = []
        for i in dataset['tag'].to_list():
            temp.extend(set(i))
        vocab = list(set(temp))
    
    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}
    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}
    return tok2idx, idx2tok


token2idx, idx2token = get_dict_map(dataset, 'token')
tag2idx, idx2tag = get_dict_map(dataset, 'tag')

address_idx = []
for index, row in dataset.iterrows():
    address_idx.append(list(map(token2idx.get, row['address_array'])))

dataset['address_idx'] = address_idx

tag_idx = []
for index, row in dataset.iterrows():
    tag_idx.append(list(map(tag2idx.get, row['tag'])))

dataset['tag_idx'] = tag_idx

dataset

from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

def get_pad_train_test_val(dataset):

    #get max token and tag length
    # n_token = len(list(set(data_group['Word'].to_list())))
    temp = []
    for i in dataset['address_array'].to_list():
        temp.extend(list(set(i)))
    n_token = len(list(set(temp)))

    # n_tag = len(list(set(data_group['Tag'].to_list())))
    temp = []
    for i in dataset['tag'].to_list():
        temp.extend(set(i))
    n_tag = len(list(set(temp)))

    #Pad tokens (X var)    
    tokens = dataset['address_idx'].tolist()
    maxlen = max([len(s) for s in tokens])
    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)

    #Pad Tags (y var) and convert it into one hot encoding
    tags = dataset['tag_idx'].tolist()
    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx["O"])
    n_tags = len(tag2idx)
    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]
    
    #Split train, test and validation set
    # tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)
    train_tokens, val_tokens, train_tags, val_tags = train_test_split(pad_tokens,pad_tags,test_size = 0.25,train_size =0.75, random_state=2020)

    print(
        'train_tokens length:', len(train_tokens),
        '\ntrain_tokens length:', len(train_tokens),
        # '\ntest_tokens length:', len(test_tokens),
        # '\ntest_tags:', len(test_tags),
        '\nval_tokens:', len(val_tokens),
        '\nval_tags:', len(val_tags),
    )
    
    return train_tokens, val_tokens, train_tags, val_tags

train_tokens, val_tokens, train_tags, val_tags = get_pad_train_test_val(dataset)

# x_train, x_validation, y_train, y_validation = train_test_split(dataset['cleaned_raw_address'], dataset['tag'], test_size=0.2, random_state=0, shuffle=True) 

# temp = []
# for i in dataset['address_array'].to_list():
#     temp.extend(list(set(i)))
# n_token = len(list(set(temp)))

# # n_tag = len(list(set(data_group['Tag'].to_list())))
# temp = []
# for i in dataset['tag'].to_list():
#     temp.extend(set(i))
# n_tag = len(list(set(temp)))

# tokens = dataset['address_idx'].tolist()
# maxlen = max([len(s) for s in tokens])

# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(list(x_train))

# x_train    =   tokenizer.texts_to_sequences(x_train) 
# x_validation   =   tokenizer.texts_to_sequences(x_validation)

# #padding zero upto maximum length
# train_tokens    =   pad_sequences(x_train,  maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)
# val_tokens   =   pad_sequences(x_validation, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)

# x_voc_size   =  len(tokenizer.word_index) +1

# y_tokenizer = Tokenizer()
# y_tokenizer.fit_on_texts(list(y_train))

# #convert summary sequences into integer sequences
# y_train    =   y_tokenizer.texts_to_sequences(y_train) 
# y_validation   =   y_tokenizer.texts_to_sequences(y_validation) 

# #padding zero upto maximum length
# train_tags    =   pad_sequences(y_train, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx["O"])
# val_tags   =   pad_sequences(y_validation, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx["O"])

# n_tags = len(tag2idx)
# # train_tags = [to_categorical(i, num_classes=n_tags) for i in train_tags]
# # val_tags = [to_categorical(i, num_classes=n_tags) for i in val_tags]

# y_voc_size  =   len(y_tokenizer.word_index) + 1

import numpy as np
import tensorflow
from tensorflow.keras import Sequential, Model, Input
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional
from tensorflow.keras.utils import plot_model

from numpy.random import seed
seed(1)
tensorflow.random.set_seed(2)

temp = []
for i in dataset['address_array'].to_list():
    temp.extend(list(set(i)))
input_dim = len(list(set(temp))) + 1
# input_dim = len(list(set(dataset['Word'].to_list())))+1
output_dim = 64
input_length = max([len(s) for s in dataset['address_idx'].tolist()])
n_tags = len(tag2idx)

print('input_dim: ', input_dim, '\noutput_dim: ', output_dim, '\ninput_length: ', input_length, '\nn_tags: ', n_tags)

"""#Modeling"""

model = Sequential()

# Add Embedding layer
model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))

# Add bidirectional LSTM
model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.2), merge_mode = 'concat'))

# Add bidirectional LSTM
model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.2), merge_mode = 'concat'))

# Add LSTM
model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))

# Add LSTM
model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))

# Add timeDistributed Layer
model.add(TimeDistributed(Dense(n_tags, activation="relu")))

#Optimiser 
optimizer = tf.keras.optimizers.RMSprop(lr=0.001)#, beta_1=0.9, beta_2=0.999)

# Compile model
model.compile(loss='mse', optimizer=optimizer, metrics=['categorical_accuracy'])
# model.summary()

hist = model.fit(train_tokens, np.array(train_tags), validation_data=(val_tokens, np.array(val_tags)), batch_size=1000, verbose=1, epochs=10)

acc = hist.history['categorical_accuracy']
val_acc = hist.history['val_categorical_accuracy']

loss = hist.history['loss']
val_loss = hist.history['val_loss']

epochs_training = range(1, len(acc)+1)

plt.plot(epochs_training, acc, 'r', label='Training accuracy')
plt.plot(epochs_training, val_acc, 'y', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()

plt.plot(epochs_training, loss, 'r', label='Training loss')
plt.plot(epochs_training, val_loss, 'y', label='Validation loss')
plt.title('Training and validation loss')
plt.legend(loc=1)
plt.figure()

from seqeval.metrics import precision_score, recall_score, f1_score, classification_report
test_tokens = val_tokens[:500*32]
test_pred = model.predict(np.array(test_tokens), verbose=1)

idx2tag_ = {i: w for w, i in tag2idx.items()}
def pred2label(pred):
    out = []
    for pred_i in pred:
        out_i = []
        for p in pred_i:
            p_i = np.argmax(p)
            # print(p_i)
            out_i.append(idx2tag_[p_i].replace("PADword", "O"))
        out.append(out_i)
    return out
def test2label(pred):
    out = []
    for pred_i in pred:
        out_i = []
        for p in pred_i:
            # print(p)
            p_i = np.argmax(p)
            out_i.append(idx2tag_[p_i].replace("PADword", "O"))
        out.append(out_i)
    return out
    
pred_labels = pred2label(test_pred)
test_labels = test2label(val_tags[:500*32])
print(classification_report(test_labels, pred_labels))
# pred_labels

i = 5000
batch_size = 32

p = model.predict(np.array(val_tokens[i:i+batch_size]))[0]
p = np.argmax(p, axis=-1)
print("{:15} {:5}: ({})".format("Word", "Pred", "True"))
print("="*30)
for w, true, pred in zip(val_tokens[i], val_tags[i], p):
    true = np.argmax(true)
    if w != "__PAD__":
        print("{:15}:{:5} ({})".format(idx2token.get(w), idx2tag.get(pred), idx2tag.get(true)))
        # print(pred)

"""#Evaluate and Test"""

testset = pd.read_csv("/content/test.csv")
testset

import string

table = str.maketrans('', '', string.punctuation)
test_address_array = []
for index, row in testset.iterrows():
    stripped = [w.translate(table) for w in row['raw_address'].split()]
    test_address_array.append(stripped)
    
testset['address_array'] = test_address_array
testset

address_idx = []
for index, row in testset.iterrows():
    address_idx.append(list(map(token2idx.get, row['address_array'])))

address_idx_temp = []
for row in address_idx:
    address_idx_temp.append([0 if i == None else i for i in row])

testset['address_idx'] = address_idx_temp
testset

temp = []
for i in testset['address_array'].to_list():
    temp.extend(list(set(i)))
n_token = len(list(set(temp)))

# n_tag = len(list(set(data_group['Tag'].to_list())))

#Pad tokens (X var)    
tokens = testset['address_idx'].tolist()
maxlen = max([len(s) for s in tokens])
test_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)

from seqeval.metrics import precision_score, recall_score, f1_score, classification_report
test_tokens = test_tokens
test_pred = model.predict(np.array(test_tokens), verbose=1)

idx2tag_ = {i: w for w, i in tag2idx.items()}
def pred2label(pred):
    out = []
    for pred_i in pred:
        out_i = []
        for p in pred_i:
            p_i = np.argmax(p)
            # print(p_i)
            out_i.append(idx2tag_[p_i].replace("PADword", "O"))
        out.append(out_i)
    return out
def test2label(pred):
    out = []
    for pred_i in pred:
        out_i = []
        for p in pred_i:
            # print(p)
            p_i = np.argmax(p)
            out_i.append(idx2tag_[p_i].replace("PADword", "O"))
        out.append(out_i)
    return out
    
pred_labels = pred2label(test_pred)
test_labels = test2label(test_tokens)
print(classification_report(test_labels, pred_labels))
# pred_labels

testset['pred_tag'] = pred_labels
testset

"""#Create Submission"""

table = str.maketrans('', '', string.punctuation)
test_address_array = []
for index, row in testset.iterrows():
    adrr = row['raw_address'].replace(',', '')
    stripped = adrr.split()
    test_address_array.append(stripped)

testset['address_array'] = test_address_array

pred_street = []
pred_POI_Street = []
for index, row in testset.iterrows():
    temp_street = []
    temp_POI = []
    length = len(row['address_array'])
    for i in range(length):
        if row['pred_tag'][i] == 'street':
            temp_street.append(row['address_array'][i])
        if row['pred_tag'][i] == 'POI':
            temp_POI.append(row['address_array'][i])
    street = ' '.join(temp_street)
    POI = ' '.join(temp_POI)
    pred_POI_Street.append(POI+'/'+street)

testset['POI/street'] = pred_POI_Street
testset

testset_temp = testset.copy()
submission = testset.drop(['raw_address', 'address_array', 'address_idx', 'pred_tag'], axis=1)

submission

submission.to_csv('/content/submission.csv', header=True, index=False)

"""#Save Model H5"""

model.save("/content/address-extraction-shopee-code-league-2021-NER-test.h5")